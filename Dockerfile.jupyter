FROM jupyter/minimal-notebook:latest

USER root

# Spark versions
ENV SPARK_VERSION=4.1.1
ENV HADOOP_VERSION=3

# Install Java 17 (Requerido para Spark 4.x)
RUN apt-get update && \
    apt-get install -y openjdk-17-jdk wget procps curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Configuración dinámica de JAVA_HOME para evitar errores de arquitectura
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Instalación de Spark
COPY deps/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz .
RUN tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
# Eliminamos la versión fija de Py4J para evitar errores de "File Not Found"
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH

# Instalación de Coursier
COPY deps/coursier /usr/local/bin/coursier
RUN chmod +x /usr/local/bin/coursier

# Volvemos al usuario de Jupyter para instalar el kernel
USER ${NB_USER}

# Instalación de Almond
# Nota: Almond 0.14.0-RC15 es muy estable para entornos modernos
RUN coursier launch --fork almond:0.14.4 --scala 2.13.17 -- \
    --install --extra-class-path "/opt/spark/jars/*"

WORKDIR /home/jovyan/work