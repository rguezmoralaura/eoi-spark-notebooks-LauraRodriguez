{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 2: Introducción a Spark con Scala\n",
    "\n",
    "## Contexto del Ejercicio: Mi Porfolio como Data Engineer\n",
    "\n",
    "Este notebook forma parte de la actividad **\"Mi porfolio como Data Engineer\"** descrita en el syllabus (páginas 18-19). \n",
    "\n",
    "El objetivo es continuar la construcción de tu **base de conocimiento** (porfolio). Puedes utilizar este notebook como:\n",
    "*   **Guía**: Para entender la arquitectura de Spark y su API.\n",
    "*   **Base**: Para experimentar con diferentes transformaciones y acciones.\n",
    "*   **Complemento**: A tu repositorio de código en GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "En esta segunda semana, nos adentraremos en el desarrollo de aplicaciones distribuidas utilizando Apache Spark. Veremos los conceptos fundamentales, trabajaramos con RDDs (Spark Core) y DataFrames (Spark SQL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Desarrollo de aplicaciones con Apache Spark\n",
    "\n",
    "Apache Spark es un motor de análisis unificado para el procesamiento de datos a gran escala.\n",
    "\n",
    "### Conceptos Clave\n",
    "*   **Driver**: El proceso principal que ejecuta tu aplicación (el `main`), crea el `SparkContext`/`SparkSession` y coordina las tareas.\n",
    "*   **Executor**: Procesos que se ejecutan en los nodos del clúster, responsables de ejecutar las tareas y almacenar datos en memoria o disco.\n",
    "*   **Cluster Manager**: Gestor de recursos (e.g., Standalone, YARN, Kubernetes) que asigna recursos a la aplicación.\n",
    "\n",
    "### Ventajas\n",
    "*   **Velocidad**: Ejecución en memoria, mucho más rápido que MapReduce tradicional.\n",
    "*   **Facilidad de uso**: APIs de alto nivel en Scala, Java, Python y R.\n",
    "*   **Unificado**: Soporta SQL, Streaming, ML y Graph en un solo motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.classic.SparkSession@142110eb"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Inicialización de SparkSession (El punto de entrada a Spark)\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Semana2_Porfolio\")\n",
    "  .master(\"local[*]\") // Ejecutar localmente usando todos los cores disponibles\n",
    "  //.master(\"spark://spark-master:7077\") // Si activas este modo obtendrás algunos errores por la integración de Ammonite y Spark\n",
    "  // Memoria del Driver (donde se recolectan los resultados de .collect())\n",
    "  .config(\"spark.driver.memory\", \"2g\") \n",
    "  // Memoria de cada Executor\n",
    "  .config(\"spark.executor.memory\", \"2g\")\n",
    "  // Memoria adicional por encima del heap (útil para evitar errores de overhead)\n",
    "  .config(\"spark.executor.memoryOverhead\", \"512m\")\n",
    "  //.config(\"deploy-mode\",\"client\")\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") // Reducir el ruido en los logs\n",
    "\n",
    "println(s\"Spark Version: ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodos activos detectados: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mstatus\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, (\u001b[32mLong\u001b[39m, \u001b[32mLong\u001b[39m)] = \u001b[33mMap\u001b[39m(\n",
       "  \u001b[32m\"967bedb628f7:33279\"\u001b[39m -> (\u001b[32m1808164454L\u001b[39m, \u001b[32m1808164454L\u001b[39m)\n",
       ")\n",
       "\u001b[36mhosts\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"967bedb628f7:33279\"\u001b[39m)\n",
       "\u001b[36mtotalMem\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mLong\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m1724L\u001b[39m)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Obtenemos el estado de los ejecutores\n",
    "// Retorna un Map: \"host:puerto\" -> (Memoria Total, Memoria Libre)\n",
    "val status = spark.sparkContext.getExecutorMemoryStatus\n",
    "\n",
    "val hosts = status.keys.toSeq\n",
    "val totalMem = status.values.map(_._1 / (1024 * 1024)).toSeq // Convertir a MB\n",
    "\n",
    "println(s\"Nodos activos detectados: ${hosts.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0.18\n",
      "17.0.18\n"
     ]
    }
   ],
   "source": [
    "// En el Driver (Jupyter)\n",
    "println(System.getProperty(\"java.version\"))\n",
    "\n",
    "// En los Workers\n",
    "spark.sparkContext.parallelize(Seq(1)).map(_ => System.getProperty(\"java.version\")).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introducción al módulo Spark Core (RDDs)\n",
    "\n",
    "RDD (Resilient Distributed Dataset) es la abstracción fundamental de Spark. Representa una colección inmutable de objetos distribuida y tolerante a fallos.\n",
    "\n",
    "### Transformaciones vs Acciones\n",
    "*   **Transformaciones (Lazy)**: Crean un nuevo RDD a partir de uno existente (ej. `map`, `filter`). No se ejecutan inmediatamente.\n",
    "*   **Acciones**: Disparan la computación y devuelven un resultado al Driver o guardan datos (ej. `count`, `collect`, `saveAsTextFile`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras con 's':\n",
      "spark\n",
      "es\n",
      "spark\n",
      "es\n",
      "scala\n",
      "spark\n",
      "es\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdatos\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"Spark es rapido\"\u001b[39m,\n",
       "  \u001b[32m\"Spark es genial\"\u001b[39m,\n",
       "  \u001b[32m\"Scala y Spark\"\u001b[39m,\n",
       "  \u001b[32m\"Big Data es el futuro\"\u001b[39m\n",
       ")\n",
       "\u001b[36mrdd\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[2] at parallelize at cmd5.sc:5\n",
       "\u001b[36mpalabrasRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[5] at filter at cmd5.sc:11\n",
       "\u001b[36mresultado\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"spark\"\u001b[39m,\n",
       "  \u001b[32m\"es\"\u001b[39m,\n",
       "  \u001b[32m\"spark\"\u001b[39m,\n",
       "  \u001b[32m\"es\"\u001b[39m,\n",
       "  \u001b[32m\"scala\"\u001b[39m,\n",
       "  \u001b[32m\"spark\"\u001b[39m,\n",
       "  \u001b[32m\"es\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Ejemplo Spark Core: Procesamiento de texto básico con RDDs\n",
    "val datos = Seq(\"Spark es rapido\", \"Spark es genial\", \"Scala y Spark\", \"Big Data es el futuro\")\n",
    "\n",
    "// 1. Crear RDD paralelizando una colección existente\n",
    "val rdd = spark.sparkContext.parallelize(datos)\n",
    "\n",
    "// 2. Transformaciones\n",
    "val palabrasRDD = rdd\n",
    "  .flatMap(linea => linea.split(\" \")) // Dividir frases en palabras\n",
    "  .map(palabra => palabra.toLowerCase) // Convertir a minúsculas\n",
    "  .filter(palabra => palabra.contains(\"s\")) // Filtrar palabras que contienen 's'\n",
    "\n",
    "// 3. Acción (Solo aquí se ejecuta el procesamiento)\n",
    "val resultado = palabrasRDD.collect()\n",
    "\n",
    "println(\"Palabras con 's':\")\n",
    "resultado.foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introducción al módulo Spark SQL (DataFrames)\n",
    "\n",
    "Spark SQL permite consultar datos estructurados. El DataFrame es la abstracción principal aquí: es como una tabla en una base de datos relacional o un DataFrame en pandas, pero distribuido.\n",
    "\n",
    "Los DataFrames utilizan el **Catalyst Optimizer** para optimizar automáticamente las consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: integer (nullable = false)\n",
      " |-- rol: string (nullable = true)\n",
      "\n",
      "+-------+----+--------------+\n",
      "| nombre|edad|           rol|\n",
      "+-------+----+--------------+\n",
      "|  Alice|  28| Data Engineer|\n",
      "|    Bob|  35|Data Scientist|\n",
      "|Charlie|  23|  Data Analyst|\n",
      "|  David|  42| Data Engineer|\n",
      "+-------+----+--------------+\n",
      "\n",
      "Data Engineers mayores de 25:\n",
      "+------+----+\n",
      "|nombre|edad|\n",
      "+------+----+\n",
      "| Alice|  28|\n",
      "| David|  42|\n",
      "+------+----+\n",
      "\n",
      "Edad promedio por rol:\n",
      "+--------------+---------+\n",
      "|           rol|avg(edad)|\n",
      "+--------------+---------+\n",
      "| Data Engineer|     35.0|\n",
      "|Data Scientist|     35.0|\n",
      "|  Data Analyst|     23.0|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[36mpersonas\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"Alice\"\u001b[39m, \u001b[32m28\u001b[39m, \u001b[32m\"Data Engineer\"\u001b[39m),\n",
       "  (\u001b[32m\"Bob\"\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m\"Data Scientist\"\u001b[39m),\n",
       "  (\u001b[32m\"Charlie\"\u001b[39m, \u001b[32m23\u001b[39m, \u001b[32m\"Data Analyst\"\u001b[39m),\n",
       "  (\u001b[32m\"David\"\u001b[39m, \u001b[32m42\u001b[39m, \u001b[32m\"Data Engineer\"\u001b[39m)\n",
       ")\n",
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [nombre: string, edad: int ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._ // Import necesario para conversiones implícitas a DF\n",
    "\n",
    "// Ejemplo Spark SQL: DataFrames\n",
    "\n",
    "// 1. Crear DataFrame desde una secuencia de tuplas\n",
    "val personas = Seq(\n",
    "  (\"Alice\", 28, \"Data Engineer\"),\n",
    "  (\"Bob\", 35, \"Data Scientist\"),\n",
    "  (\"Charlie\", 23, \"Data Analyst\"),\n",
    "  (\"David\", 42, \"Data Engineer\")\n",
    ")\n",
    "\n",
    "val df = personas.toDF(\"nombre\", \"edad\", \"rol\")\n",
    "\n",
    "// 2. Mostrar el esquema y los datos\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "// 3. Consultas usando API de DataFrame\n",
    "println(\"Data Engineers mayores de 25:\")\n",
    "df.filter($\"rol\" === \"Data Engineer\" && $\"edad\" > 25)\n",
    "  .select(\"nombre\", \"edad\")\n",
    "  .show()\n",
    "\n",
    "// 4. Agregaciones\n",
    "println(\"Edad promedio por rol:\")\n",
    "df.groupBy(\"rol\")\n",
    "  .avg(\"edad\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|           rol|total|\n",
      "+--------------+-----+\n",
      "| Data Engineer|    2|\n",
      "|Data Scientist|    1|\n",
      "|  Data Analyst|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msqlDF\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [rol: string, total: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 5. Consultas SQL estándar\n",
    "// Registramos el DataFrame como una vista temporal\n",
    "df.createOrReplaceTempView(\"personas_view\")\n",
    "\n",
    "val sqlDF = spark.sql(\"SELECT rol, count(*) as total FROM personas_view GROUP BY rol\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|edad|\n",
      "+------+----+\n",
      "| David|  42|\n",
      "|   Bob|  35|\n",
      "+------+----+\n",
      "\n",
      "+--------------+------------+----------+--------+\n",
      "|           rol|num_personas|edad_media|edad_max|\n",
      "+--------------+------------+----------+--------+\n",
      "| Data Engineer|           2|      35.0|      42|\n",
      "|Data Scientist|           1|      35.0|      35|\n",
      "|  Data Analyst|           1|      23.0|      23|\n",
      "+--------------+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//*****    PROPIOS lAURA   *****//\n",
    " \n",
    "// Ejemplo de data frame con Select + filtro\n",
    "df.select($\"nombre\", $\"edad\")\n",
    "  .where($\"edad\" >= 29)\n",
    "  .orderBy($\"edad\".desc)\n",
    "  .show()\n",
    "\n",
    "// Agregaciones\n",
    "df.groupBy($\"rol\")\n",
    "  .agg(\n",
    "    count(lit(1)).as(\"num_personas\"),\n",
    "    avg($\"edad\").as(\"edad_media\"),\n",
    "    max($\"edad\").as(\"edad_max\")\n",
    "  )\n",
    "  .orderBy($\"num_personas\".desc)\n",
    "  .show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejercicios Prácticos (Sin resolver)\n",
    "\n",
    "Usa estos ejercicios como base para practicar y documentar en tu porfolio.\n",
    "\n",
    "### Ejercicio 1: Manipulación de DataFrames\n",
    "Crea un DataFrame a partir de una lista de productos (nombre, precio, stock). Luego:\n",
    "1. Añade una columna `valor_inventario` (precio * stock).\n",
    "2. Filtra los productos que tengan un stock menor a 10.\n",
    "3. Muestra el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+----------------+\n",
      "| nombre|precio|stock|valor_inventario|\n",
      "+-------+------+-----+----------------+\n",
      "| Laptop|1200.0|    5|          6000.0|\n",
      "|Teclado|  45.0|    8|           360.0|\n",
      "+-------+------+-----+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[36mproductos\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"Laptop\"\u001b[39m, \u001b[32m1200.0\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "  (\u001b[32m\"Mouse\"\u001b[39m, \u001b[32m25.0\u001b[39m, \u001b[32m20\u001b[39m),\n",
       "  (\u001b[32m\"Teclado\"\u001b[39m, \u001b[32m45.0\u001b[39m, \u001b[32m8\u001b[39m),\n",
       "  (\u001b[32m\"Monitor\"\u001b[39m, \u001b[32m300.0\u001b[39m, \u001b[32m15\u001b[39m),\n",
       "  (\u001b[32m\"USB\"\u001b[39m, \u001b[32m10.0\u001b[39m, \u001b[32m50\u001b[39m)\n",
       ")\n",
       "\u001b[36mdfProductos\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [nombre: string, precio: double ... 1 more field]\n",
       "\u001b[36mdfConValor\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [nombre: string, precio: double ... 2 more fields]\n",
       "\u001b[36mdfResultado\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [nombre: string, precio: double ... 2 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// 1. Creo la lista de productos con su nombre, precio y stock.\n",
    "val productos = Seq(\n",
    "  (\"Laptop\", 1200.0, 5),\n",
    "  (\"Mouse\", 25.0, 20),\n",
    "  (\"Teclado\", 45.0, 8),\n",
    "  (\"Monitor\", 300.0, 15),\n",
    "  (\"USB\", 10.0, 50)\n",
    ")\n",
    "\n",
    "// 2. A partir de dicha lista, creo el dataframe\n",
    "val dfProductos = productos.toDF(\"nombre\", \"precio\", \"stock\")\n",
    "\n",
    "// 3. Añado una nueva columna al dataframe \"valor_inventario\" que tendra el valor de multiplciar el precio por el stock.\n",
    "val dfConValor = dfProductos.withColumn(\n",
    "  \"valor_inventario\",\n",
    "  col(\"precio\") * col(\"stock\")\n",
    ")\n",
    "\n",
    "// 4. Filtro los productos con stock menor a 10\n",
    "val dfResultado = dfConValor.filter(col(\"stock\") < 10)\n",
    "\n",
    "// 5. Muestro el resultado\n",
    "dfResultado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Consultas SQL\n",
    "Registra el DataFrame de productos anterior como una vista temporal y realiza una consulta SQL que devuelva el precio medio de los productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|precio_medio|\n",
      "+------------+\n",
      "|       316.0|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfMedia\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [precio_medio: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 1. Registro el DataFrame de productos como una vista temporal\n",
    "dfProductos.createOrReplaceTempView(\"productos\")\n",
    "\n",
    "// 2. Ejecuto la consulta SQL para calcular el precio medio\n",
    "val dfMedia = spark.sql(\"\"\"\n",
    "  SELECT AVG(precio) AS precio_medio\n",
    "  FROM productos\n",
    "\"\"\")\n",
    "\n",
    "// 3. Muestro el resultado\n",
    "dfMedia.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Windows functions: ranking por cicudad\n",
    "\n",
    "Objetivo: Tener un ejemplo de uso de de la función windows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------+-------------+\n",
      "| nombre|edad|           rol|rank_edad_rol|\n",
      "+-------+----+--------------+-------------+\n",
      "|Charlie|  23|  Data Analyst|            1|\n",
      "|  David|  42| Data Engineer|            1|\n",
      "|  Alice|  28| Data Engineer|            2|\n",
      "|    Bob|  35|Data Scientist|            1|\n",
      "+-------+----+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m\n",
       "\u001b[36mw\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@262f15ab"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val w = Window.partitionBy($\"rol\").orderBy($\"edad\".desc)\n",
    "\n",
    "df.withColumn(\"rank_edad_rol\", dense_rank().over(w))\n",
    "  .orderBy($\"rol\", $\"rank_edad_rol\")\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. UDF: Ejemplo de uso de UDF\n",
    "\n",
    "UDF signfica función definida por el usuario.\n",
    "Uso: Se suele usarla dentro de Spark SQL o DataFrames cuando las funciones estándar de Spark no son suficientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|rango_edad|count|\n",
      "+----------+-----+\n",
      "|    senior|    2|\n",
      "|    adulto|    1|\n",
      "|     joven|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36maRangoEdad\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  f = ammonite.$sess.cmd14$Helper$$Lambda$6430/0x00007efd9d7251a0@6f0dd0a6,\n",
       "  dataType = StringType,\n",
       "  inputEncoders = \u001b[33mArraySeq\u001b[39m(\u001b[33mSome\u001b[39m(value = PrimitiveIntEncoder)),\n",
       "  outputEncoder = \u001b[33mSome\u001b[39m(value = StringEncoder),\n",
       "  givenName = \u001b[32mNone\u001b[39m,\n",
       "  nullable = \u001b[32mtrue\u001b[39m,\n",
       "  deterministic = \u001b[32mtrue\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// función que segun el palametro de entrada 'edad' lo evalua y duelve una cadena\n",
    "val aRangoEdad = udf((edad: Int) =>\n",
    "  if (edad < 25) \"joven\"\n",
    "  else if (edad < 35) \"adulto\"\n",
    "  else \"senior\"\n",
    ")\n",
    "\n",
    "\n",
    "//Creo una columna en el dataframe 'rango_edad' y le calculo los valores haciendo uso de la función que acabo de definir.\n",
    "df.withColumn(\"rango_edad\", aRangoEdad($\"edad\"))\n",
    "  .groupBy(\"rango_edad\")\n",
    "  .count()\n",
    "  .orderBy(desc(\"count\"))\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lectura/escritura: Parquet\n",
    "\n",
    "Parquet es un formato de almacenamiento de datos diseñado para Big Data. \n",
    "Guarda los datos por columnas, no por filas, lo que hace las consultas mucho más rápidas y eficientes.\n",
    "\r\n",
    "Ejemplo local (crea carpeta en el proyecto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------+\n",
      "| nombre|edad|           rol|\n",
      "+-------+----+--------------+\n",
      "|Charlie|  23|  Data Analyst|\n",
      "|  Alice|  28| Data Engineer|\n",
      "|  David|  42| Data Engineer|\n",
      "|    Bob|  35|Data Scientist|\n",
      "+-------+----+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36moutPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"output/personas_parquet\"\u001b[39m\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [nombre: string, edad: int ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outPath = \"output/personas_parquet\"\n",
    "\n",
    "// Escritura\n",
    "df.write.mode(\"overwrite\").parquet(outPath)\n",
    "\n",
    "// Lectura\n",
    "val df2 = spark.read.parquet(outPath)\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejemplo IOT\n",
    "\n",
    "**Objetivo:** analizar telemetría de dispositivos IoT (temperatura/humedad) y:\n",
    "\n",
    " 1. Marcar alertas (temperatura alta/baja, humedad alta) \n",
    " 2. Calcular métricas por tipo de dispositivo \n",
    " 3. Calcular métricas por ventanas de tiempo (por minuto)\n",
    " 4. Guardar resultados en Parquet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo creado con éxito en: /home/jovyan/work/data/datagen/iot_data.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\u001b[39m\n",
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/datagen/\"\u001b[39m\n",
       "\u001b[36mfileName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"iot_data.json\"\u001b[39m\n",
       "\u001b[36mcontenido\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"{\"deviceId\": 1, \"temperature\": 22.5, \"humidity\": 45.0, \"timestamp\": 1708976088, \"deviceType\": \"sensor_temp\"}\n",
       "{\"deviceId\": 2, \"temperature\": 38.2, \"humidity\": 82.1, \"timestamp\": 1708976148, \"deviceType\": \"sensor_temp\"}\n",
       "{\"deviceId\": 3, \"temperature\": -5.0, \"humidity\": 30.0, \"timestamp\": 1708976208, \"deviceType\": \"industrial\"}\n",
       "{\"deviceId\": 4, \"temperature\": 20.0, \"humidity\": 90.0, \"timestamp\": 1708976268, \"deviceType\": \"industrial\"}\"\"\"\u001b[39m\n",
       "\u001b[36mdirectory\u001b[39m: \u001b[32mFile\u001b[39m = data/datagen\n",
       "\u001b[36mres4_5\u001b[39m: \u001b[32mAnyVal\u001b[39m = ()\n",
       "\u001b[36mfile\u001b[39m: \u001b[32mFile\u001b[39m = data/datagen/iot_data.json\n",
       "\u001b[36mbw\u001b[39m: \u001b[32mBufferedWriter\u001b[39m = java.io.BufferedWriter@260938d8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io._\n",
    "\n",
    "//ME CREO UN FICHERO DE PRUEBA PARA PROBAR EL EJMPLO DE IOT.\n",
    "\n",
    "// 1. Degino la ruta donde se guardará.\n",
    "val path = \"data/datagen/\"\n",
    "val fileName = \"iot_data.json\"\n",
    "\n",
    "// 2. Creo el contenido simulado (formato JSON)\n",
    "val contenido = \"\"\"{\"deviceId\": 1, \"temperature\": 22.5, \"humidity\": 45.0, \"timestamp\": 1708976088, \"deviceType\": \"sensor_temp\"}\n",
    "{\"deviceId\": 2, \"temperature\": 38.2, \"humidity\": 82.1, \"timestamp\": 1708976148, \"deviceType\": \"sensor_temp\"}\n",
    "{\"deviceId\": 3, \"temperature\": -5.0, \"humidity\": 30.0, \"timestamp\": 1708976208, \"deviceType\": \"industrial\"}\n",
    "{\"deviceId\": 4, \"temperature\": 20.0, \"humidity\": 90.0, \"timestamp\": 1708976268, \"deviceType\": \"industrial\"}\"\"\"\n",
    "\n",
    "// 3. Creo directorio y lo guardo.\n",
    "val directory = new File(path)\n",
    "if (!directory.exists()) directory.mkdirs()\n",
    "\n",
    "val file = new File(path + fileName)\n",
    "val bw = new BufferedWriter(new FileWriter(file))\n",
    "bw.write(contenido)\n",
    "bw.close()\n",
    "\n",
    "println(s\"Archivo creado con éxito en: ${file.getAbsolutePath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- MUESTRA DE DATOS CON ALERTAS ---------\n",
      "+--------+-----------+--------+-----------+-------------------+----------+\n",
      "|deviceId|deviceType |humidity|temperature|eventTime          |alert     |\n",
      "+--------+-----------+--------+-----------+-------------------+----------+\n",
      "|1       |sensor_temp|45.0    |22.5       |2024-02-26 19:34:48|OK        |\n",
      "|2       |sensor_temp|82.1    |38.2       |2024-02-26 19:35:48|TEMP_HIGH |\n",
      "|3       |industrial |30.0    |-5.0       |2024-02-26 19:36:48|TEMP_LOW  |\n",
      "|4       |industrial |90.0    |20.0       |2024-02-26 19:37:48|HUMID_HIGH|\n",
      "+--------+-----------+--------+-----------+-------------------+----------+\n",
      "\n",
      "\n",
      "--------- MÉTRICAS POR TIPO ---------\n",
      "+-----------+--------+--------+--------+------------+--------+\n",
      "|deviceType |n_events|avg_temp|max_temp|avg_humidity|n_alerts|\n",
      "+-----------+--------+--------+--------+------------+--------+\n",
      "|industrial |2       |7.5     |20.0    |60.0        |2       |\n",
      "|sensor_temp|2       |30.35   |38.2    |63.55       |1       |\n",
      "+-----------+--------+--------+--------+------------+--------+\n",
      "\n",
      "\n",
      "--------- MÉTRICAS POR MINUTO ---------\n",
      "+------------------------------------------+-----------+--------+--------+--------+\n",
      "|minute_window                             |deviceType |n_events|avg_temp|n_alerts|\n",
      "+------------------------------------------+-----------+--------+--------+--------+\n",
      "|{2024-02-26 19:35:00, 2024-02-26 19:36:00}|sensor_temp|1       |38.2    |1       |\n",
      "|{2024-02-26 19:36:00, 2024-02-26 19:37:00}|industrial |1       |-5.0    |1       |\n",
      "|{2024-02-26 19:37:00, 2024-02-26 19:38:00}|industrial |1       |20.0    |1       |\n",
      "|{2024-02-26 19:34:00, 2024-02-26 19:35:00}|sensor_temp|1       |22.5    |0       |\n",
      "+------------------------------------------+-----------+--------+--------+--------+\n",
      "\n",
      "\n",
      ">>> Proceso completado con éxito. Archivos guardados en 'output/'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.classic.SparkSession@7e71bec9\n",
       "\u001b[36mIotJsonDataPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/datagen/iot_data.json\"\u001b[39m\n",
       "\u001b[36miotRaw\u001b[39m: \u001b[32mDataFrame\u001b[39m = [deviceId: bigint, deviceType: string ... 3 more fields]\n",
       "\u001b[36miot\u001b[39m: \u001b[32mDataFrame\u001b[39m = [deviceId: int, deviceType: string ... 3 more fields]\n",
       "\u001b[36miotWithAlerts\u001b[39m: \u001b[32mDataFrame\u001b[39m = [deviceId: int, deviceType: string ... 4 more fields]\n",
       "\u001b[36mmetricsByType\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [deviceType: string, n_events: bigint ... 4 more fields]\n",
       "\u001b[36mmetricsByMinute\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [minute_window: struct<start: timestamp, end: timestamp>, deviceType: string ... 3 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// 1. Inicializo la Sesión de Spark\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"IoT Analytics Notebook\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Año esta linea para que no salga el texto en rosa que da la sensación de error.\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "// 2. Configuro la ruta donde esta el fichero json con los datos de los sensores (creando anteriormente)\n",
    "val IotJsonDataPath = \"data/datagen/iot_data.json\"\n",
    "\n",
    "// 3. Leo los datos del fichero y los \"limpio\"\n",
    "val iotRaw = spark.read.json(IotJsonDataPath)\n",
    "\n",
    "val iot = iotRaw\n",
    "  .withColumn(\"deviceId\", col(\"deviceId\").cast(IntegerType))\n",
    "  .withColumn(\"temperature\", col(\"temperature\").cast(DoubleType))\n",
    "  .withColumn(\"humidity\", col(\"humidity\").cast(DoubleType))\n",
    "  .withColumn(\"eventTime\", to_timestamp(from_unixtime(col(\"timestamp\"))))\n",
    "  .drop(\"timestamp\")\n",
    "\n",
    "// 4. Aplico reglas para las alertas\n",
    "val iotWithAlerts = iot.withColumn(\n",
    "  \"alert\",\n",
    "  when(col(\"temperature\") >= 35, lit(\"TEMP_HIGH\"))\n",
    "    .when(col(\"temperature\") <= 0, lit(\"TEMP_LOW\"))\n",
    "    .when(col(\"humidity\") >= 80, lit(\"HUMID_HIGH\"))\n",
    "    .otherwise(lit(\"OK\"))\n",
    ")\n",
    "\n",
    "println(\"\\n--------- MUESTRA DE DATOS CON ALERTAS ---------\")\n",
    "iotWithAlerts.show(10, truncate = false)\n",
    "\n",
    "// 5. Agrego las metricas por Tipo de Dispositivo\n",
    "val metricsByType = iotWithAlerts\n",
    "  .groupBy(\"deviceType\")\n",
    "  .agg(\n",
    "    count(lit(1)).as(\"n_events\"),\n",
    "    round(avg(col(\"temperature\")), 2).as(\"avg_temp\"),\n",
    "    round(max(col(\"temperature\")), 2).as(\"max_temp\"),\n",
    "    round(avg(col(\"humidity\")), 2).as(\"avg_humidity\"),\n",
    "    sum(when(col(\"alert\") =!= \"OK\", 1).otherwise(0)).as(\"n_alerts\")\n",
    "  )\n",
    "  .orderBy(desc(\"n_alerts\"))\n",
    "\n",
    "println(\"\\n--------- MÉTRICAS POR TIPO ---------\")\n",
    "metricsByType.show(truncate = false)\n",
    "\n",
    "// 6. Ejmplo de análisis por Ventanas de tiempo (1 minuto)\n",
    "val metricsByMinute = iotWithAlerts\n",
    "  .groupBy(\n",
    "    window(col(\"eventTime\"), \"1 minute\").as(\"minute_window\"),\n",
    "    col(\"deviceType\")\n",
    "  )\n",
    "  .agg(\n",
    "    count(lit(1)).as(\"n_events\"),\n",
    "    round(avg(col(\"temperature\")), 2).as(\"avg_temp\"),\n",
    "    sum(when(col(\"alert\") =!= \"OK\", 1).otherwise(0)).as(\"n_alerts\")\n",
    "  )\n",
    "  .orderBy(desc(\"n_alerts\"))\n",
    "\n",
    "println(\"\\n--------- MÉTRICAS POR MINUTO ---------\")\n",
    "metricsByMinute.show(10, truncate = false)\n",
    "\n",
    "// 7. Finalmente, guardado los resultados en formato Parquet\n",
    "metricsByType.write.mode(\"overwrite\").parquet(\"output/iot/metrics_by_type\")\n",
    "metricsByMinute.write.mode(\"overwrite\").parquet(\"output/iot/metrics_by_minute\")\n",
    "\n",
    "println(\"\\n>>> Proceso completado con éxito. Archivos guardados en 'output/'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
